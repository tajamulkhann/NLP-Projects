# ğŸ“– Evolution of NLP Approaches

![alt text](image.png)

---

## 1ï¸âƒ£ Classical Machine Learning (Feature Engineering Era)

**Goal**: Represent text numerically and apply ML models.  

**Techniques**:  
- **OHE (One Hot Encoding)** â€“ sparse, no meaning between words.  
- **BoW (Bag of Words)** â€“ counts word frequencies, loses order.  
- **TF-IDF** â€“ weighted counts (rarer words get higher importance).  
- **Word2Vec / GloVe** â€“ word embeddings capturing semantic meaning (e.g., king â€“ man + woman â‰ˆ queen).  

âš ï¸ **Limitation**: These are **context-independent embeddings** (same word = same vector, regardless of sentence).  

---

## 2ï¸âƒ£ Deep Learning (Sequential Models Era)

Introduced **neural networks for sequences** to capture context.  

- **Simple RNN** â€“ learns short dependencies, but vanishing gradient â†’ canâ€™t capture long context.  
- **LSTM** â€“ solves long-term memory with gates.  
- **GRU** â€“ simplified LSTM, fewer gates, faster.  
- **Bi-directional RNN** â€“ looks at both past and future in the sequence.  

âš ï¸ **Limitation**: **Sequential processing** â†’ slow training, struggles with very long sequences.  

---

## 3ï¸âƒ£ Seq2Seq Models (Encoderâ€“Decoder Era)

### ğŸ”¹ Vanilla Encoderâ€“Decoder (Seq2Seq)  
- Encoder compresses input into a single **context vector**.  
- Decoder generates output step by step.  
- ğŸš¨ **Issue**: Bottleneck â†’ long sentences lose information.  

### ğŸ”¹ Seq2Seq with Attention  
- Decoder attends to **all encoder hidden states** instead of just context vector.  
- Dynamically computes a **context vector per step**.  
- âœ… Huge improvement in **translation, summarization**.  

---

## 4ï¸âƒ£ Transformers (Attention-only Era)

- Introduced in **â€œAttention Is All You Needâ€ (2017)**.  
- **Key Idea**: Drop recurrence â†’ use only **self-attention**.  

**Advantages**:  
- Parallelizable (**fast training**).  
- Captures **long-range dependencies** better.  
- Stacks multiple layers of **self-attention + feedforward blocks**.  

**Variants**:  
- **BERT** â†’ encoder-only (understanding tasks like classification, QA).  
- **GPT** â†’ decoder-only (generation tasks).  
- **T5 / BART** â†’ encoderâ€“decoder (translation, summarization).  

âš¡ Today, **Transformers dominate NLP** (translation, chatbots, summarization, coding assistants, etc.).  

---

## âœ… Final Evolution Path
**ML features â†’ RNNs â†’ Seq2Seq â†’ Attention-enhanced Seq2Seq â†’ Transformers**
